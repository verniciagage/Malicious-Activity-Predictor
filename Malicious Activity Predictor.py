# -*- coding: utf-8 -*-
"""Analytics_Project_DIII_USF1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ML5WKg3gkEO-R7IACF_conJWi-soNPFi

Classification model

Deliverable I
"""

#Importing libraries 

from google.colab import files
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import pandas as pd
import io
import numpy as np; np.random.seed(42)
import plotly.express as px

import statsmodels.api as sm1
import statsmodels.regression.linear_model as sm2
import statsmodels as sm3
from sklearn.model_selection import train_test_split
import statsmodels.formula.api as sm4
import statsmodels.api as sm
import seaborn as sns
from sklearn import preprocessing
import seaborn as sns 
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.impute import KNNImputer
from google.colab import drive
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import KFold  #instead of splitting data into testing or training just in kfolds.
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import classification_report
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

#we loaded the data from google drive 
drive.mount('/content/gdrive')

!ls "/content/gdrive/My Drive/Colab Notebooks/testing_data_2_predictor_values.csv"

#reading the training data set 
data=pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/training_data_2.csv')
data

#we can see the detailes of each predictor column
data.describe()

#count how many 0 and 1 we have
# 0 is not malicious and 1 is malicious
data['Response'].value_counts()

"""Data Visualization"""

#plotting response values, malicious vs not malicious
sns.countplot(x=data['Response'], palette='hls')

plt.hist(data['Response'],bins=10)

#Correlation between predictor variables
data.corr().style.background_gradient(cmap='coolwarm')
#HPositive numbers mean positive correlation, negative numbers means negative correlation, smaller numbers mean less correlation.

#box plots to visualize outliers 
np.random.seed(42)
boxplots=pd.DataFrame(data = data, columns = ['Node1','Node2','Node3','Node4','Node5','Node6'])
boxplots.boxplot()

#we need to standardize

#dropping activity column from data set
X=data.drop('Activity',axis=1)

#plotting every predictor grouping it by response
X.groupby('Response').hist(figsize=(200, 200), color='blue')

#check if we have NaN values data preprocessing
#MISSING VALUES
X.isnull()

#sum how many nan values are present
X.isnull().any().sum()

#finding nan in the data set
df1 = X[X.isna().any(axis=1)]
df1

#DrOPPING NAN values from the data set
X=X.dropna()
X

#check if all nan are dropped
X.isnull().any().sum()

#Drop response and activity column and separate predictors from response

y=(X['Response'])
y
x=X.drop('Response',axis=1)
x
# x-----> predictors
# y-----> response

##standarizing the values
#centering and scaling
x=StandardScaler().fit_transform(x)
x
x=pd.DataFrame(x)
x

#Splitting data into testing and training 
X_train, X_test, Y_train, Y_test = train_test_split(x, y, stratify = X.Response, random_state=0)

"""### Robust Models
Deliverable III Downsampling and Upsampling to handle unbalanced dataset

Down Sampling
"""

# Python code to apply down-sampling technique
# Comment it out to disable the downsampling
# Uncomment to downsample the data
from imblearn.under_sampling import NearMiss
nm = NearMiss(n_neighbors=5)
X_train, Y_train = nm.fit_sample(X_train,Y_train)

"""Upsampling"""

# Code to upsample the training dataset
# Comment it out to disable the upsampling
# Uncomment to upsample the data
#from imblearn.over_sampling import SMOTE
#sm= SMOTE(k_neighbors=5)
#X_train, Y_train= sm.fit_sample(X_train,Y_train)

"""Model selection logistic regression"""

#running logistic regression
logregmodel=LogisticRegression()
logregmodel.fit(X_train,Y_train)

#prediction from test data
Y_pred = logregmodel.predict(X_test)
Y_pred

#Coefficient of determination
r2_score(Y_test,Y_pred)

#create classification report 
print(classification_report(Y_test, Y_pred))

# manual calculation of sensitivity 
sensitivity=39912/(39912+3279)
sensitivity

# manual calculation of specificity 
specificity=1230/(1230+564)
specificity

#confusion matrix
confu_matrix = confusion_matrix(Y_test, Y_pred)
print(confu_matrix)

log_misclass = (confu_matrix[1,0] + confu_matrix[0,1])/sum(sum(confu_matrix))
print('--------Misclassification Rate--------')
print(log_misclass)

#Test accuracy for the model
accuracy=accuracy_score(Y_test,Y_pred)
accuracy

#ROC Curve for the model
log_roc_auc = roc_auc_score(Y_test, Y_pred)
fpr, tpr, thresholds = roc_curve(Y_test, Y_pred)
plt.figure()
plt.plot(fpr, tpr, label='Reduced Logistic Regression (area = %0.2f)' % log_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

# the same model with the same data with Cross Validation giving almost the same accuracy.
kfold = KFold(n_splits=5, random_state=0) 
score_cv = cross_val_score(logregmodel, x, y, cv=kfold, scoring='accuracy').mean()
score_cv

#comparing the actual values vs the predicted ones in a table
df = pd.DataFrame({'Actual':Y_test, 'Predicted': Y_pred})
df

#Dimension reduction with PCA 
from sys import stdout
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import savgol_filter
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import mean_squared_error, r2_score
from sklearn import model_selection
from sklearn.preprocessing import scale

"""Dimensional Reduction PCA"""

from sklearn.decomposition import PCA

#PCA
pca2 = PCA()

# Scale the data
X_reduced_train = pca2.fit_transform(scale(X_train))
n = len(X_reduced_train)

# 10-fold CV, with shuffle
kf_10 = model_selection.KFold( n_splits=10, shuffle=True, random_state=1)
regr = logregmodel
accuracy_pca = []

# Calculate Misclass with only the intercept (no principal components in regression)
score = model_selection.cross_val_score(regr, np.ones((n,1)), Y_train.ravel(), cv=kf_10, scoring='accuracy').mean()    
accuracy_pca.append(score)

# Calculate accuracy using CV for the principle components, adding one component at the time.
for i in np.arange(1, 201): #change the number 10 to however many components to analyze. if you want to do it for all 200 components --->np.arange(1, 201)
    score = model_selection.cross_val_score(regr, X_reduced_train[:,:i], Y_train.ravel(), cv=kf_10, scoring='accuracy').mean()
    print(i)
    
    accuracy_pca.append(score)

plt.plot(np.array(accuracy_pca), '-v')
plt.xlabel('Number of principal components in regression')
plt.ylabel('Accuracy')
plt.title('')
plt.xlim(xmin=-1)

print(score)
print(accuracy_pca)
print(max(accuracy_pca))
print(accuracy_pca[2])

X_reduced_test = pca2.transform(scale(X_test))[:,:3]

# Train regression model on training data 
regr = logregmodel
regr.fit(X_reduced_train[:,:3], Y_train)

# Prediction with test data
pred = pd.DataFrame(regr.predict(X_reduced_test))
mean_squared_error(Y_test, pred)
#print(pred)

#--,R^2,ROC,ACCURACY,MATRIX,SENSITIVITY, Specificity
print('--------Accuracy--------')
print(accuracy_score(Y_test, pred))
print('--------rsquared--------')
print(r2_score(Y_test, pred))
print('--------Confusion Matrix--------')
reduced_confu_matrix = confusion_matrix(Y_test, pred)
print(reduced_confu_matrix)
reducedlog_misclass = (reduced_confu_matrix[1,0] + reduced_confu_matrix[0,1])/sum(sum(reduced_confu_matrix))
print('--------Misclassification Rate--------')
print(reducedlog_misclass)
print('--------Classification Report--------')
reduced_class_report = classification_report(Y_test, pred, output_dict=True)
print(classification_report(Y_test, pred))
print('--------Sensitivity--------')
reduced_sensitivity = reduced_class_report['0']['precision']
print(reduced_sensitivity)
print('--------Specificity--------')
reduced_specificity = reduced_class_report['1']['precision']
print(reduced_specificity)

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
log_reduced_roc_auc = roc_auc_score(Y_test, pred)
fpr, tpr, thresholds = roc_curve(Y_test, pred)
plt.figure()
plt.plot(fpr, tpr, label='Reduced Logistic Regression (area = %0.2f)' % log_reduced_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

"""Running Test Data"""

#For test data file
testing_data=pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/testing_data_2_predictor_values.csv')
# check if there are any null values in the data
testing_data.isnull().any().sum()
testing_data=testing_data.drop("Activity",axis=1)
testing_data

x_testing_data=StandardScaler().fit_transform(testing_data)
x_testing_data=pd.DataFrame(x_testing_data)
print(x_testing_data)

x_testing_data

# Deliverable I prediction
# Uncomment to output prediction
#prediction from test data
#X_reduced_testing = pca2.transform(scale(x_testing_data))[:,:3]
#Y_testing_pred = regr.predict(X_reduced_testing)

#import numpy as np 
#np.savetxt("predictions.csv", Y_testing_pred)
#files.download('predictions.csv')

"""Deliverable II"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import make_classification

"""Adaboost classifier"""

#Running the Adaboost classifier with 100 estimators
model_AB= AdaBoostClassifier(n_estimators=100,learning_rate=1)
model_AB.fit(X_train, Y_train)

#predicting the y_hat values
y_pred = model_AB.predict(X_test)

#calculating the accuracy of this model
accuracy_score(Y_test, Y_pred)

#Using KFold cross validation to test our adaboost model
kfold=KFold(n_splits=10,random_state=0,shuffle=True)

score_cv=cross_val_score(model_AB,X_train,Y_train,cv=kfold,scoring='accuracy').mean()

score_cv.mean()
score_cv

#making the confusion matrix
ConfusionMatrix_AM=confusion_matrix(Y_test,y_pred)
print(ConfusionMatrix_AM)

misclass_Ab= (ConfusionMatrix_AM[1,0] + ConfusionMatrix_AM[0,1])/sum(sum(ConfusionMatrix_AM))

print('--------Misclassification Rate--------')

print(misclass_Ab)

#printing the classification report
print(classification_report(Y_test,y_pred))

#sensitivity of the model
sensitivity1 = ConfusionMatrix_AM[0,0]/(ConfusionMatrix_AM[0,0]+ConfusionMatrix_AM[1,0])
print('Sensitivity : ', sensitivity1 )

#specificity of the model
specificity1 = ConfusionMatrix_AM[1,1]/(ConfusionMatrix_AM[0,1]+ConfusionMatrix_AM[1,1])
print('Specificity : ', specificity1)

#Here is the ROC curve. 
AB_roc_auc = roc_auc_score(Y_test, y_pred)
fpr, tpr, thresholds = roc_curve(Y_test, y_pred)
plt.figure()
plt.plot(fpr, tpr, label=' Adaboost (area = %0.2f)' % AB_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('AB_ROC')
plt.show()

"""LSVC model"""

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn.datasets import make_classification
from sklearn.model_selection import GridSearchCV

#making the LSVC model
model_LSVC = GridSearchCV(LinearSVC(random_state=0, tol=1e-5, max_iter=1000), param_grid= {'C': [0.1, 1.0, 10, 100]})

#fitting the model
model_LSVC.fit(X_train, Y_train)

#printing the best parameters values
print(model_LSVC.best_params_)

#calculating the y_hat values of the model
y_hat_LSVC = model_LSVC.best_estimator_.predict(X_test)

#confusion matrix
confu_matrix_LSVC = confusion_matrix(Y_test, y_hat_LSVC)
print('--------Confusion Matrix--------')
print(confu_matrix_LSVC)

#Classification Report, Sensitivity, Specificity, Misclassification Rate
print('--------Classification Report--------')
class_report_LSVC = classification_report(Y_test, y_hat_LSVC, output_dict=True)
print(classification_report(Y_test, y_hat_LSVC))

print('--------Sensitivity--------')
sensitivity_LSVC = class_report_LSVC['0']['precision']
print(sensitivity_LSVC)

print('--------Specificity--------')
specificity_LSVC = class_report_LSVC['1']['precision']
print(specificity_LSVC)

misclass_LSVC = (confu_matrix_LSVC[1,0] + confu_matrix_LSVC[0,1])/sum(sum(confu_matrix_LSVC))
print('--------Misclassification Rate--------')
print(misclass_LSVC)

#ROC Curve for the model
roc_auc_LSVC = roc_auc_score(Y_test, y_hat_LSVC)
fpr, tpr, thresholds = roc_curve(Y_test, y_hat_LSVC)
plt.figure()
plt.plot(fpr, tpr, label='LSVC (area = %0.2f)' % roc_auc_LSVC)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('LSVC_ROC')
plt.show()

# Deliverable II prediction
# Uncomment to output prediction
#prediction from test data based on the PCA model because has the less missclassification error.

#X_reduced_testing = pca2.transform(scale(x_testing_data))[:,:3]
#Y_testing_pred = regr.predict(X_reduced_testing)

#import numpy as np 
#np.savetxt("predictions2.csv", Y_testing_pred)
#files.download('predictions2.csv')

"""Deliverable III"""

from sklearn.neural_network import MLPClassifier

model=MLPClassifier()
model.fit(X_train,Y_train)
y_hat_mlp=model.predict(X_test)
from sklearn.metrics import confusion_matrix,accuracy_score
confusion_matrix(Y_test,y_hat_mlp)
accuracy_score(Y_test,y_hat_mlp)

#confusion matrix
confu_matrix_NN = confusion_matrix(Y_test, y_hat_mlp)
print('--------Confusion Matrix--------')
print(confu_matrix_NN)

#Classification Report, Sensitivity, Specificity, Misclassification Rate
print('--------Classification Report--------')
class_report = classification_report(Y_test, y_hat_mlp, output_dict=True)
print(classification_report(Y_test, y_hat_mlp))

print('--------Sensitivity--------')
sensitivity = class_report['0']['precision']
print(sensitivity)

print('--------Specificity--------')
specificity = class_report['1']['precision']
print(specificity)

misclass = (confu_matrix_NN[1,0] + confu_matrix_NN[0,1])/sum(sum(confu_matrix_NN))
print('--------Misclassification Rate--------')
print(misclass)

#ROC Curve for the model
roc_auc = roc_auc_score(Y_test, y_hat_mlp)
fpr, tpr, thresholds = roc_curve(Y_test, y_hat_mlp)
plt.figure()
plt.plot(fpr, tpr, label=' (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('ROC')
plt.show()

#For test data file
imb_testing_data=pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/imbalanced_testing_data_2_predictor_values.csv')
# check if there are any null values in the data
imb_testing_data.isnull().any().sum()
imb_testing_data=imb_testing_data.drop("Activity",axis=1)
imb_testing_data

x_imb_testing_data=StandardScaler().fit_transform(imb_testing_data)
x_imb_testing_data=pd.DataFrame(x_imb_testing_data)
print(x_imb_testing_data)

x_imb_testing_data

# Deliverable III prediction

#prediction from test data

Y_imb_pred =  model_LSVC.best_estimator_.predict(X_imb_testing_data)

import numpy as np 
np.savetxt("imb_predictions.csv", Y_imb_pred)
files.download('imb_predictions.csv')